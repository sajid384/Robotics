"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[75],{7699(n,e,i){i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>m,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var s=i(4848),t=i(8453);const a={sidebar_position:7,title:"Chapter 7: Simulation Fundamentals for Humanoid Robots"},l="Chapter 7: Simulation Fundamentals for Humanoid Robots",o={id:"part_iii/chapter_7",title:"Chapter 7: Simulation Fundamentals for Humanoid Robots",description:"Learning Objectives",source:"@site/docs/part_iii/chapter_7.md",sourceDirName:"part_iii",slug:"/part_iii/chapter_7",permalink:"/Robotics/docs/part_iii/chapter_7",draft:!1,unlisted:!1,editUrl:"https://github.com/syedsajidhussain/Robotics/tree/main/docs/part_iii/chapter_7.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7,title:"Chapter 7: Simulation Fundamentals for Humanoid Robots"},sidebar:"docs",previous:{title:"Chapter 6: Control and Actuation in ROS 2",permalink:"/Robotics/docs/part_ii/chapter_6"},next:{title:"Chapter 8: Creating Digital Twins",permalink:"/Robotics/docs/part_iii/chapter_8"}},r={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Vision-Language-Action (VLA) Framework",id:"vision-language-action-vla-framework",level:3},{value:"Vision Processing Component",id:"vision-processing-component",level:3},{value:"Language Understanding Component",id:"language-understanding-component",level:3},{value:"Action Execution Component",id:"action-execution-component",level:3},{value:"Human-Robot Interaction Principles",id:"human-robot-interaction-principles",level:3},{value:"Architecture Diagrams (Described in Text)",id:"architecture-diagrams-described-in-text",level:2},{value:"Architecture 1: Basic VLA Integration",id:"architecture-1-basic-vla-integration",level:3},{value:"Architecture 2: Multimodal Perception System",id:"architecture-2-multimodal-perception-system",level:3},{value:"Architecture 3: Humanoid Control Pipeline",id:"architecture-3-humanoid-control-pipeline",level:3},{value:"Hands-on Labs",id:"hands-on-labs",level:2},{value:"Lab 1: Setting Up VLA Environment",id:"lab-1-setting-up-vla-environment",level:3},{value:"Lab 2: Vision Processing Integration",id:"lab-2-vision-processing-integration",level:3},{value:"Lab 3: Language Understanding Implementation",id:"lab-3-language-understanding-implementation",level:3},{value:"Lab 4: Action Planning and Execution",id:"lab-4-action-planning-and-execution",level:3},{value:"Lab 5: Integrated VLA System",id:"lab-5-integrated-vla-system",level:3},{value:"Lab 6: Advanced Humanoid Control",id:"lab-6-advanced-humanoid-control",level:3},{value:"Toolchain: Vision Processing, Language Models, Humanoid Control",id:"toolchain-vision-processing-language-models-humanoid-control",level:2},{value:"Vision Processing Setup",id:"vision-processing-setup",level:3},{value:"Language Model Integration",id:"language-model-integration",level:3},{value:"Humanoid Control Framework",id:"humanoid-control-framework",level:3},{value:"Integration Tools",id:"integration-tools",level:3},{value:"Failure Modes &amp; Debugging Checklist",id:"failure-modes--debugging-checklist",level:2},{value:"Common Vision Processing Issues",id:"common-vision-processing-issues",level:3},{value:"Common Language Understanding Issues",id:"common-language-understanding-issues",level:3},{value:"Common Action Execution Issues",id:"common-action-execution-issues",level:3},{value:"Integration Issues",id:"integration-issues",level:3},{value:"Performance Issues",id:"performance-issues",level:3},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Knowledge Assessment",id:"knowledge-assessment",level:3},{value:"Practical Assessment",id:"practical-assessment",level:3},{value:"Performance Assessment",id:"performance-assessment",level:3}];function d(n){const e={h1:"h1",h2:"h2",h3:"h3",input:"input",li:"li",ol:"ol",p:"p",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.h1,{id:"chapter-7-simulation-fundamentals-for-humanoid-robots",children:"Chapter 7: Simulation Fundamentals for Humanoid Robots"}),"\n",(0,s.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Understand the integration of Vision-Language-Action (VLA) systems for humanoid robot control"}),"\n",(0,s.jsx)(e.li,{children:"Learn how to create AI systems that interpret natural language commands and execute corresponding physical actions"}),"\n",(0,s.jsx)(e.li,{children:"Explore the architecture of unified perception, language, and motor control systems"}),"\n",(0,s.jsx)(e.li,{children:"Gain hands-on experience with VLA implementation for human-robot interaction"}),"\n",(0,s.jsx)(e.li,{children:"Master the coordination between visual perception, language understanding, and action execution"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,s.jsx)(e.h3,{id:"vision-language-action-vla-framework",children:"Vision-Language-Action (VLA) Framework"}),"\n",(0,s.jsx)(e.p,{children:"The VLA framework represents a unified approach to artificial intelligence that combines visual perception, natural language understanding, and physical action execution in a single coherent system (Driess et al., 2023). This approach enables humanoid robots to interpret human commands in natural language while perceiving their environment visually and executing appropriate physical responses (Brohan & Burdick, 2008)."}),"\n",(0,s.jsx)(e.h3,{id:"vision-processing-component",children:"Vision Processing Component"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Image acquisition and preprocessing (Szeliski, 2010)"}),"\n",(0,s.jsx)(e.li,{children:"Scene understanding and object recognition (Russell & Norvig, 2020)"}),"\n",(0,s.jsx)(e.li,{children:"Spatial reasoning and environment mapping (Thrun et al., 2005)"}),"\n",(0,s.jsx)(e.li,{children:"Visual attention mechanisms (Itti & Koch, 2001)"}),"\n",(0,s.jsx)(e.li,{children:"Real-time visual processing for robotics (Marszalek et al., 2009)"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"language-understanding-component",children:"Language Understanding Component"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Natural language processing for command interpretation"}),"\n",(0,s.jsx)(e.li,{children:"Semantic parsing and intent recognition"}),"\n",(0,s.jsx)(e.li,{children:"Context awareness and memory systems"}),"\n",(0,s.jsx)(e.li,{children:"Dialogue management for human-robot interaction"}),"\n",(0,s.jsx)(e.li,{children:"Multimodal language grounding in visual context"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"action-execution-component",children:"Action Execution Component"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"High-level command translation to low-level motor controls"}),"\n",(0,s.jsx)(e.li,{children:"Motion planning and trajectory generation"}),"\n",(0,s.jsx)(e.li,{children:"Real-time control and feedback systems"}),"\n",(0,s.jsx)(e.li,{children:"Safety protocols and emergency responses"}),"\n",(0,s.jsx)(e.li,{children:"Adaptive control for dynamic environments"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"human-robot-interaction-principles",children:"Human-Robot Interaction Principles"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Natural command interfaces"}),"\n",(0,s.jsx)(e.li,{children:"Situational awareness and context understanding"}),"\n",(0,s.jsx)(e.li,{children:"Intuitive response mechanisms"}),"\n",(0,s.jsx)(e.li,{children:"Error handling and clarification requests"}),"\n",(0,s.jsx)(e.li,{children:"Collaborative task execution"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"architecture-diagrams-described-in-text",children:"Architecture Diagrams (Described in Text)"}),"\n",(0,s.jsx)(e.h3,{id:"architecture-1-basic-vla-integration",children:"Architecture 1: Basic VLA Integration"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Human Command Input \u2192 Language Understanding \u2192 Vision Processing \u2192 Action Planning \u2192 Motor Execution \u2192 Humanoid Robot"}),"\n",(0,s.jsx)(e.li,{children:"Feedback loops for confirmation and error correction"}),"\n",(0,s.jsx)(e.li,{children:"Real-time perception-action coupling"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"architecture-2-multimodal-perception-system",children:"Architecture 2: Multimodal Perception System"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Camera Input + Microphone Input \u2192 Sensor Fusion \u2192 VLA Model \u2192 Command Execution"}),"\n",(0,s.jsx)(e.li,{children:"Cross-modal attention mechanisms"}),"\n",(0,s.jsx)(e.li,{children:"Joint visual-language reasoning pathways"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"architecture-3-humanoid-control-pipeline",children:"Architecture 3: Humanoid Control Pipeline"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Visual Scene Analysis \u2192 Language Command Interpretation \u2192 Action Sequence Planning \u2192 Motor Control Execution \u2192 Physical Robot Response"}),"\n",(0,s.jsx)(e.li,{children:"Safety monitoring and validation layers"}),"\n",(0,s.jsx)(e.li,{children:"Human feedback integration and adaptation"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"hands-on-labs",children:"Hands-on Labs"}),"\n",(0,s.jsx)(e.h3,{id:"lab-1-setting-up-vla-environment",children:"Lab 1: Setting Up VLA Environment"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Install vision processing libraries (OpenCV, etc.)"}),"\n",(0,s.jsx)(e.li,{children:"Configure language model interfaces"}),"\n",(0,s.jsx)(e.li,{children:"Set up humanoid robot control framework"}),"\n",(0,s.jsx)(e.li,{children:"Verify sensor and actuator connections"}),"\n",(0,s.jsx)(e.li,{children:"Test basic perception-action loop"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"lab-2-vision-processing-integration",children:"Lab 2: Vision Processing Integration"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Configure camera and sensor inputs"}),"\n",(0,s.jsx)(e.li,{children:"Implement object detection and recognition"}),"\n",(0,s.jsx)(e.li,{children:"Set up spatial reasoning systems"}),"\n",(0,s.jsx)(e.li,{children:"Test visual scene understanding"}),"\n",(0,s.jsx)(e.li,{children:"Validate perception accuracy"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"lab-3-language-understanding-implementation",children:"Lab 3: Language Understanding Implementation"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Set up NLP pipeline for command processing"}),"\n",(0,s.jsx)(e.li,{children:"Configure semantic parsing systems"}),"\n",(0,s.jsx)(e.li,{children:"Implement intent recognition algorithms"}),"\n",(0,s.jsx)(e.li,{children:"Test command interpretation accuracy"}),"\n",(0,s.jsx)(e.li,{children:"Validate language grounding in visual context"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"lab-4-action-planning-and-execution",children:"Lab 4: Action Planning and Execution"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Create command-to-action mapping system"}),"\n",(0,s.jsx)(e.li,{children:"Implement motion planning algorithms"}),"\n",(0,s.jsx)(e.li,{children:"Configure motor control interfaces"}),"\n",(0,s.jsx)(e.li,{children:"Test basic action execution"}),"\n",(0,s.jsx)(e.li,{children:"Validate safety protocols"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"lab-5-integrated-vla-system",children:"Lab 5: Integrated VLA System"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Combine vision, language, and action components"}),"\n",(0,s.jsx)(e.li,{children:"Test end-to-end command execution"}),"\n",(0,s.jsx)(e.li,{children:"Implement feedback and error handling"}),"\n",(0,s.jsx)(e.li,{children:"Validate human-robot interaction scenarios"}),"\n",(0,s.jsx)(e.li,{children:"Optimize system performance"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"lab-6-advanced-humanoid-control",children:"Lab 6: Advanced Humanoid Control"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Implement complex multi-step commands"}),"\n",(0,s.jsx)(e.li,{children:"Test collaborative task execution"}),"\n",(0,s.jsx)(e.li,{children:"Configure adaptive behavior systems"}),"\n",(0,s.jsx)(e.li,{children:"Validate safety and reliability"}),"\n",(0,s.jsx)(e.li,{children:"Document system limitations and capabilities"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"toolchain-vision-processing-language-models-humanoid-control",children:"Toolchain: Vision Processing, Language Models, Humanoid Control"}),"\n",(0,s.jsx)(e.h3,{id:"vision-processing-setup",children:"Vision Processing Setup"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Camera and sensor configuration"}),"\n",(0,s.jsx)(e.li,{children:"Image processing pipeline implementation"}),"\n",(0,s.jsx)(e.li,{children:"Object detection and recognition models"}),"\n",(0,s.jsx)(e.li,{children:"Spatial mapping and navigation systems"}),"\n",(0,s.jsx)(e.li,{children:"Real-time processing optimization"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"language-model-integration",children:"Language Model Integration"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"NLP framework installation and configuration"}),"\n",(0,s.jsx)(e.li,{children:"Pre-trained language model setup"}),"\n",(0,s.jsx)(e.li,{children:"Command parsing and interpretation systems"}),"\n",(0,s.jsx)(e.li,{children:"Context management and memory systems"}),"\n",(0,s.jsx)(e.li,{children:"Dialogue management interfaces"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"humanoid-control-framework",children:"Humanoid Control Framework"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Robot operating system (ROS/ROS2) setup"}),"\n",(0,s.jsx)(e.li,{children:"Motor control and actuator interfaces"}),"\n",(0,s.jsx)(e.li,{children:"Kinematic and dynamic modeling"}),"\n",(0,s.jsx)(e.li,{children:"Motion planning and trajectory generation"}),"\n",(0,s.jsx)(e.li,{children:"Safety and emergency stop systems"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"integration-tools",children:"Integration Tools"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Sensor fusion algorithms"}),"\n",(0,s.jsx)(e.li,{children:"Multimodal data processing"}),"\n",(0,s.jsx)(e.li,{children:"Real-time communication protocols"}),"\n",(0,s.jsx)(e.li,{children:"System monitoring and debugging tools"}),"\n",(0,s.jsx)(e.li,{children:"Performance optimization utilities"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"failure-modes--debugging-checklist",children:"Failure Modes & Debugging Checklist"}),"\n",(0,s.jsx)(e.h3,{id:"common-vision-processing-issues",children:"Common Vision Processing Issues"}),"\n",(0,s.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Camera not responding - Check hardware connections and drivers"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Object detection failures - Verify lighting conditions and model parameters"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Visual tracking instability - Adjust tracking algorithms and parameters"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Spatial mapping errors - Recalibrate sensors and verify coordinate systems"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"common-language-understanding-issues",children:"Common Language Understanding Issues"}),"\n",(0,s.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Command misinterpretation - Review semantic parsing and context"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Natural language processing failures - Check model availability and input format"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Context loss during interaction - Verify memory and state management"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Ambiguous command resolution - Improve disambiguation systems"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"common-action-execution-issues",children:"Common Action Execution Issues"}),"\n",(0,s.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Motor control failures - Check actuator connections and safety systems"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Motion planning errors - Review collision detection and path planning"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Trajectory execution problems - Adjust control parameters and timing"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Safety system activation - Verify safety parameters and emergency protocols"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"integration-issues",children:"Integration Issues"}),"\n",(0,s.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","VLA model response delays - Optimize model inference and processing"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Multimodal data synchronization - Check timing and data alignment"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","System stability problems - Profile and optimize resource usage"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Human-robot interaction failures - Review interface and feedback systems"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"performance-issues",children:"Performance Issues"}),"\n",(0,s.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Real-time operation failures - Optimize processing pipelines and algorithms"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Memory usage problems - Monitor and optimize resource allocation"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Communication latency - Check network and communication protocols"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","System reliability concerns - Implement redundancy and error handling"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,s.jsx)(e.h3,{id:"knowledge-assessment",children:"Knowledge Assessment"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Explain the VLA framework and its components"}),"\n",(0,s.jsx)(e.li,{children:"Describe the architecture connecting vision, language, and action systems"}),"\n",(0,s.jsx)(e.li,{children:"Identify advantages and limitations of VLA-based humanoid control"}),"\n",(0,s.jsx)(e.li,{children:"Discuss human-robot interaction principles in VLA systems"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"practical-assessment",children:"Practical Assessment"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Successfully set up the complete VLA toolchain"}),"\n",(0,s.jsx)(e.li,{children:"Create a functional vision-language-action system for humanoid control"}),"\n",(0,s.jsx)(e.li,{children:"Demonstrate natural language command execution with physical robot"}),"\n",(0,s.jsx)(e.li,{children:"Troubleshoot and resolve common failure modes using the debugging checklist"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"performance-assessment",children:"Performance Assessment"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Complete hands-on labs within specified time limits"}),"\n",(0,s.jsx)(e.li,{children:"Achieve 85% success rate in command interpretation and execution"}),"\n",(0,s.jsx)(e.li,{children:"Demonstrate understanding through problem-solving exercises"}),"\n",(0,s.jsx)(e.li,{children:"Successfully complete at least 3 failure mode resolution scenarios"}),"\n",(0,s.jsx)(e.li,{children:"Show proficiency in human-robot interaction scenarios"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>l,x:()=>o});var s=i(6540);const t={},a=s.createContext(t);function l(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:l(n.components),s.createElement(a.Provider,{value:e},n.children)}}}]);